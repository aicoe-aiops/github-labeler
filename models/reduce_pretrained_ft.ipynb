{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e64b488-1b14-44ef-826b-fb72d1d26543",
   "metadata": {},
   "source": [
    "# Reduce Pretrained fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febd817e-9c10-4cdf-af83-725e26a46e4a",
   "metadata": {},
   "source": [
    "This is a short, simple notebook that reduces the size of our pretrained fastText model. We do this by overlapping the large vocabulary set we have with all the words mentioned at least once in our target dataset. After reducing the vocabulary size with that method, we use PCA reduction to reduce our vector size. We then save our word vector file so the fastText model can access it in training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe12055-00e7-4b32-9dad-696975879eae",
   "metadata": {},
   "source": [
    "First we import our packages and load in our dataset and Word2Vec model from Ceph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d5b5079-10cd-47e0-9a9a-315c8016a6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /opt/app-\n",
      "[nltk_data]     root/src/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /opt/app-\n",
      "[nltk_data]     root/src/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /opt/app-\n",
      "[nltk_data]     root/src/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import boto3\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "\n",
    "vocab_path = \"../src/data\"\n",
    "if vocab_path not in sys.path:\n",
    "    sys.path.insert(1, vocab_path)\n",
    "\n",
    "from preprocess import preprocess # noqa\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7874c852-239f-4ceb-a218-fa0e77418a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16f6d906-fe21-40e8-8d51-47831630f003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whether to use ceph or store locally\n",
    "\n",
    "use_ceph = bool(int(os.getenv('USE_CEPH')))\n",
    "\n",
    "if use_ceph:\n",
    "    s3_endpoint_url = os.environ[\"OBJECT_STORAGE_ENDPOINT_URL\"]\n",
    "    s3_access_key = os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
    "    s3_secret_key = os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "    s3_bucket = os.environ[\"OBJECT_STORAGE_BUCKET_NAME\"]\n",
    "\n",
    "    s3 = boto3.client(\n",
    "        service_name=\"s3\",\n",
    "        aws_access_key_id=s3_access_key,\n",
    "        aws_secret_access_key=s3_secret_key,\n",
    "        endpoint_url=s3_endpoint_url,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe725f3e-f3d4-47fc-ba38-6b0ded199522",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = os.getenv(\"REPO_NAME\")\n",
    "\n",
    "if \"/\" in name:\n",
    "    REPO = name\n",
    "    USER = \"\"\n",
    "else:\n",
    "    USER = name\n",
    "    REPO = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d75ef8be-211b-44a7-b9e7-0e7f89bd9d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repo data is saved as {org_name}-_-{repo_name}\n",
    "# orginization data is saved as {org_name}\n",
    "\n",
    "savename = USER if USER else REPO.replace(\"/\", \"-_-\")\n",
    "path = os.path.join(\"../data\", savename + \".csv\")\n",
    "key = f\"github-labeler/data/{savename}.csv\"\n",
    "\n",
    "if use_ceph:\n",
    "    response = s3.get_object(Bucket=s3_bucket, Key=key)\n",
    "    issues_df = pd.read_csv(response.get(\"Body\")).drop_duplicates()\n",
    "else:\n",
    "    issues_df = pd.read_csv(path).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e91aab84-124f-4d93-821f-a111e4fda50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile('github-labeler/w2v/.*')\n",
    "\n",
    "buck = boto3.resource(\n",
    "    service_name=\"s3\",\n",
    "    aws_access_key_id=s3_access_key,\n",
    "    aws_secret_access_key=s3_secret_key,\n",
    "    endpoint_url=s3_endpoint_url,\n",
    ")\n",
    "\n",
    "keys = []\n",
    "\n",
    "for obj in buck.Bucket(s3_bucket).objects.all():\n",
    "    if pattern.match(obj.key):\n",
    "        keys.append(obj.key)\n",
    "\n",
    "keys = [os.path.basename(key) for key in keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68e550b7-bd47-4525-a7ad-129748ee8dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ceph:\n",
    "    for key in keys:\n",
    "        response = s3.get_object(\n",
    "            Bucket=s3_bucket,\n",
    "            Key=f\"github-labeler/w2v/{key}\",\n",
    "        )\n",
    "        with open(f'../models/{key}' ,'wb') as f:\n",
    "            for i in response['Body']:\n",
    "                f.write(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb275f69-951b-4365-a514-9a7a263c1be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Word2Vec.load('../models/w2v.model')\n",
    "full_vocab = set(w.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b645b50-1b9b-4e00-a836-7f7e43be6411",
   "metadata": {},
   "source": [
    "Now we generate all the words that are mentioned in our dataset and intersect it with the pre-trained Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "249e18b9-9ca3-4959-807b-7860e7f119a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = Counter()\n",
    "\n",
    "for i, row in issues_df.iterrows():\n",
    "    words = word_tokenize(preprocess(row.title + \" SEP \" + row.body if type(row.body) == str else row.title))\n",
    "    cnt.update(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae6aa4c4-8dd0-4022-98e4-bc5c4c72bcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_vocab = set(cnt).intersection(full_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc256239-4a72-43a3-a5cc-3ffcf3388d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13622 words in reduced model\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(shared_vocab)} words in reduced model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a2dffe0-cbbf-4a95-b40f-3218893e3d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_vocab_dict = {}\n",
    "for v in shared_vocab:\n",
    "    shared_vocab_dict[v] = w.wv[v]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4400c08b-8fb7-4c98-9931-4812075c2b89",
   "metadata": {},
   "source": [
    "We perform PCA reduction such that we retain 70% of the total singular value sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7bd035a-6c51-401b-a083-cccbccca575f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 dimensions in reduced model\n"
     ]
    }
   ],
   "source": [
    "pca_in = np.stack([b for _, b in shared_vocab_dict.items()])\n",
    "words = [a for a, _ in shared_vocab_dict.items()]\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(pca_in)\n",
    "sum_svs = sum(pca.singular_values_)\n",
    "dim = 0\n",
    "running = 0\n",
    "for i in pca.singular_values_:\n",
    "    dim += 1\n",
    "    running += i\n",
    "    if running/sum_svs > 0.7:\n",
    "        break\n",
    "\n",
    "print(f'{dim} dimensions in reduced model')\n",
    "\n",
    "pca = PCA(n_components=dim)\n",
    "pca_out = pca.fit_transform(pca_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af315e88-3955-4e18-bc30-464ab0c0b8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab_reduced.vec', 'w', encoding = 'utf-8') as f:\n",
    "    f.write(str(len(shared_vocab)) + ' ' + str(dim) + '\\n')\n",
    "    for w, vec in zip(words, pca_out):\n",
    "        vector = [str(v) for v in vec]\n",
    "        f.write(w + ' ' + ' '.join(vector))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b51f1c-e067-40d0-ab95-a24852e551d6",
   "metadata": {},
   "source": [
    "We upload this reduced vector file to Ceph and delete the large Word2Vec files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "baf4339a-51fd-4e6b-8e2e-5127b8bba6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ceph:\n",
    "    s3.upload_file(\n",
    "        Bucket=s3_bucket,\n",
    "        Key=f\"github-labeler/{savename}/vocab_reduced.vec\",\n",
    "        Filename=\"vocab_reduced.vec\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7bb1b52-e853-46ae-b36f-3fcd7c8b7982",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ceph:\n",
    "    w2v_pattern = re.compile('w2v.*')\n",
    "    for item in os.listdir('../models/'):\n",
    "        if re.match(w2v_pattern, item):\n",
    "            os.remove(os.path.join('../models', item))\n",
    "\n",
    "    os.remove('vocab_reduced.vec')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
