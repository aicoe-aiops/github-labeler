{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83fe8f02",
   "metadata": {},
   "source": [
    "This is a short notebook to load in a pretrained w2v model and train it on general github data. The vocabulary is built and model is initialized in the src/data/build_w2v_vocab notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bdd07e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/atersaak/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import boto3\n",
    "import sys\n",
    "\n",
    "vocab_path = \"../src/data\"\n",
    "if vocab_path not in sys.path:\n",
    "    sys.path.insert(1, vocab_path)\n",
    "\n",
    "from w2v_preprocess import remove_quotes, is_bot, is_english, preprocess, is_punc # noqa\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfbc44a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52782d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whether to use ceph or store locally\n",
    "\n",
    "use_ceph = True\n",
    "\n",
    "if use_ceph:\n",
    "    s3_endpoint_url = os.environ[\"OBJECT_STORAGE_ENDPOINT_URL\"]\n",
    "    s3_access_key = os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
    "    s3_secret_key = os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "    s3_bucket = os.environ[\"OBJECT_STORAGE_BUCKET_NAME\"]\n",
    "\n",
    "    s3 = boto3.client(\n",
    "        service_name=\"s3\",\n",
    "        aws_access_key_id=s3_access_key,\n",
    "        aws_secret_access_key=s3_secret_key,\n",
    "        endpoint_url=s3_endpoint_url,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa07df6",
   "metadata": {},
   "source": [
    "Below we copy over a couple functions from the vocab building notebook that could not easily be copied over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72a5f42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save key .json file in the github labeler root\n",
    "# project id on bigquery account should match\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    '../github-issue-data-extraction-key.json')\n",
    "\n",
    "project_id = 'github-issue-data-extraction'\n",
    "client = bigquery.Client(credentials= credentials, project=project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b290cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_day(day):\n",
    "    \"\"\"\n",
    "    Pass in a datetime object and a dataframe of all the issue data from that day will be returned.\n",
    "    \"\"\"\n",
    "    date = day.strftime('%Y%m%d')\n",
    "    response = client.query(f\"\"\"SELECT JSON_EXTRACT(payload, '$.issue.title') as title,\n",
    "                                JSON_EXTRACT(payload, '$.issue.body') as body,\n",
    "                                JSON_EXTRACT(payload, '$.issue.html_url') as url,\n",
    "                                JSON_EXTRACT(payload, '$.issue.user.login') as actor\n",
    "                                FROM githubarchive.day.{date}\n",
    "                                WHERE type = 'IssuesEvent' AND JSON_EXTRACT(payload, '$.action') = '\"opened\"'\n",
    "                                \"\"\")\n",
    "    df = response.to_dataframe()\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_df(df):\n",
    "    \"\"\"Clean the dataframe a bit.\"\"\"\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].apply(remove_quotes)\n",
    "        df = df[~df[col].apply(is_bot)]\n",
    "    df = df[~df[col].apply(is_bot)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b217d4",
   "metadata": {},
   "source": [
    "First we load in the previous data. Note it saves as multiple files so we must load all of them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a189ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile('github-labeler/w2v/.*')\n",
    "\n",
    "buck = boto3.resource(\n",
    "    service_name=\"s3\",\n",
    "    aws_access_key_id=s3_access_key,\n",
    "    aws_secret_access_key=s3_secret_key,\n",
    "    endpoint_url=s3_endpoint_url,\n",
    ")\n",
    "\n",
    "keys = []\n",
    "\n",
    "for obj in buck.Bucket(s3_bucket).objects.all():\n",
    "    if pattern.match(obj.key):\n",
    "        keys.append(obj.key)\n",
    "\n",
    "keys = [os.path.basename(key) for key in keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6de4496",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if use_ceph:\n",
    "    for key in keys:\n",
    "        response = s3.get_object(\n",
    "            Bucket=s3_bucket,\n",
    "            Key=f\"github-labeler/w2v/{key}\",\n",
    "        )\n",
    "        with open(f'../models/{key}' ,'wb') as f:\n",
    "            for i in response['Body']:\n",
    "                f.write(i)\n",
    "\n",
    "w = Word2Vec.load('../models/w2v.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f18224d",
   "metadata": {},
   "source": [
    "We define some useful functions, namely a function to filter unknown words, a function to save w2v models, and a training function to train our model for a given day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab20d28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_set(word):\n",
    "    \"\"\"Check if the word is in our set.\"\"\"\n",
    "    if word in w.wv:\n",
    "        return word\n",
    "    else:\n",
    "        return '_unknown_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7c70099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_w2v(w):\n",
    "    \"\"\"Save your w2v model to ceph or just c.\"\"\"\n",
    "    w.save('../models/w2v.model')\n",
    "    if use_ceph:\n",
    "        for file in os.listdir('../models/'):\n",
    "            if 'w2v.model' in file:\n",
    "                s3.upload_file(\n",
    "                    Bucket=s3_bucket,\n",
    "                    Key=f\"github-labeler/w2v/{file}\",\n",
    "                    Filename=f'../models/{file}',\n",
    "                )\n",
    "                os.remove(f'../models/{file}')\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce2ca285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_w2v_on_day(w, day):\n",
    "    df = get_data_for_day(day)\n",
    "    df = process_df(df)\n",
    "    df['proc'] = df['title'].fillna(' ') + ' SEP ' + df['body'].fillna(' ')\n",
    "    df['proc'] = df['proc'].apply(preprocess)\n",
    "    df = df[df['proc'].apply(is_english)]\n",
    "    df['proc'] = df['proc'].apply(lambda x: x.lower())\n",
    "    inp = df['proc'].apply(word_tokenize).values\n",
    "    inp = [[in_set(word) for word in issue if not is_punc(word)] for issue in inp]\n",
    "    w.train(inp, total_examples = len(inp), epochs = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c74d09",
   "metadata": {},
   "source": [
    "We will be training the model on random days in a certain interval. We will save the range of these dates as well as the days we have already trained on. This is because the process will likely get interrupted before finishing, especially if being run locally. We come up with methods to save and retrieve the dates locally or from ceph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "520f940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if dates file exists\n",
    "\n",
    "dates_exist = True\n",
    "\n",
    "if use_ceph:\n",
    "    try:\n",
    "        s3.get_object(\n",
    "            Bucket=s3_bucket,\n",
    "            Key=\"github-labeler/w2v_dates.txt\",\n",
    "        )\n",
    "        with open('w2v_dates.txt' ,'wb') as f:\n",
    "            for i in response['Body']:\n",
    "                f.write(i)\n",
    "    except s3.exceptions.NoSuchKey:\n",
    "        dates_exist = False\n",
    "\n",
    "else:\n",
    "    if not os.path.isfile('w2v_dates.txt'):\n",
    "        dates_exist = False\n",
    "\n",
    "# read them in if they exist\n",
    "\n",
    "if dates_exist:\n",
    "    with open('w2v_dates.txt', 'r') as f:\n",
    "        dates = f.readlines()\n",
    "        dates = [d.replace('\\n', '') for d in dates if d]\n",
    "        dates = [datetime.datetime.strptime(d, '%Y-%m-%d') for d in dates]\n",
    "\n",
    "# start 10 days ago and go back 2 years\n",
    "\n",
    "else:\n",
    "    interval_end = datetime.datetime.today().date() - datetime.timedelta(days = 10)\n",
    "    days = 365*2\n",
    "    all_days = [interval_end - datetime.timedelta(days = num) for num in range(days)]\n",
    "\n",
    "\n",
    "def save_dates(days):\n",
    "    \"\"\"Convert a list of datetimes to strings and save them.\"\"\"\n",
    "    with open('w2v_dates.txt', 'w') as f:\n",
    "        days = [datetime.datetime.strftime(d, '%Y-%m-%d') for d in days]\n",
    "        f.write('\\n'.join(days))\n",
    "    if use_ceph:\n",
    "        s3.upload_file(\n",
    "            Bucket=s3_bucket,\n",
    "            Key=\"github-labeler/w2v_dates.txt\",\n",
    "            Filename='w2v_dates.txt',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8b5ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = len(all_days)\n",
    "\n",
    "while all_days:\n",
    "    random_day = np.random.choice(all_days)\n",
    "    all_days.remove(random_day)\n",
    "    train_w2v_on_day(w, random_day)\n",
    "    if len(all_days) % 3 == 0:\n",
    "        print(f'{initial - len(all_days)} days trained on')\n",
    "        save_w2v(w)\n",
    "        save_dates(all_days)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
