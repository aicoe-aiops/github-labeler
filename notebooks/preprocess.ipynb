{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "662f187e",
   "metadata": {},
   "source": [
    "# SVM Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585d38c7",
   "metadata": {},
   "source": [
    "This is a notebook to do some basic preprocessing of a given dataset for training the SVM models specifically. The fastText model preprocessing is different and will be done in a different notebook. \n",
    "\n",
    "The goal of this notebook is to preprocess the text and build a smaller vocabulary that will be recognized by the SVM models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35078c3",
   "metadata": {},
   "source": [
    "## Environment Variables and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6fc042",
   "metadata": {},
   "source": [
    "First we import packages and load in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bd9facb-b2a3-42d9-9692-2e80eca414d9",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from string import punctuation\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import boto3\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bc5e20e-ae30-4047-9f04-fdd327589401",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18fc248d-95be-4b0d-a924-9b50a35b0109",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "use_ceph = bool(int(os.getenv('USE_CEPH')))\n",
    "\n",
    "if use_ceph:\n",
    "    s3_endpoint_url = os.environ[\"OBJECT_STORAGE_ENDPOINT_URL\"]\n",
    "    s3_access_key = os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
    "    s3_secret_key = os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "    s3_bucket = os.environ[\"OBJECT_STORAGE_BUCKET_NAME\"]\n",
    "\n",
    "    # Create an S3 client\n",
    "    s3 = boto3.client(\n",
    "        service_name=\"s3\",\n",
    "        aws_access_key_id=s3_access_key,\n",
    "        aws_secret_access_key=s3_secret_key,\n",
    "        endpoint_url=s3_endpoint_url,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3c28f4c-067d-4a41-8e9a-a5a39caae64c",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "name = os.getenv(\"REPO_NAME\")\n",
    "\n",
    "if \"/\" in name:\n",
    "    REPO = name\n",
    "    USER = \"\"\n",
    "else:\n",
    "    USER = name\n",
    "    REPO = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47c12d33-110d-4111-91fe-56bb2b7a42a8",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/openshift-_-origin.csv\n"
     ]
    }
   ],
   "source": [
    "# repo data is saved as {org_name}-_-{repo_name}\n",
    "# orginization data is saved as {org_name}\n",
    "\n",
    "savename = USER if USER else REPO.replace(\"/\", \"-_-\")\n",
    "path = os.path.join(\"../data\", savename + \".csv\")\n",
    "key = f\"github-labeler/data/{savename}.csv\"\n",
    "\n",
    "if use_ceph:\n",
    "    response = s3.get_object(Bucket=s3_bucket, Key=key)\n",
    "    issues_df = pd.read_csv(response.get(\"Body\")).drop_duplicates()\n",
    "else:\n",
    "    issues_df = pd.read_csv(path).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9562b2",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72696748",
   "metadata": {},
   "source": [
    "We now define a handful of preprocessing functions which will combine to be one preprocessing function. Most of these functions are simple regex expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b8e71e8-ff44-4762-921f-baea97195bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_list = []\n",
    "\n",
    "pattern = r\"```.+?```\"\n",
    "code_block_regex = re.compile(pattern, re.DOTALL)\n",
    "\n",
    "\n",
    "def code_block(string):\n",
    "    \"\"\"\n",
    "    replaces code blocks with a CODE_BLOCK\n",
    "    \"\"\"\n",
    "    string = re.sub(code_block_regex, \"CODE_BLOCK\", string)\n",
    "    return string\n",
    "\n",
    "\n",
    "function_list.append(code_block)\n",
    "\n",
    "pattern = r\"`{1,2}.+?`{1,2}\"\n",
    "inline_code_regex = re.compile(pattern, re.DOTALL)\n",
    "\n",
    "\n",
    "def code_variable(string):\n",
    "    \"\"\"\n",
    "    replaces inline code with VARIABLE\n",
    "    \"\"\"\n",
    "    string = re.sub(inline_code_regex, \" INLINE \", string)\n",
    "    return string\n",
    "\n",
    "\n",
    "function_list.append(code_variable)\n",
    "\n",
    "pattern = r\"\\s@[^\\s]+\"\n",
    "tagged_user_regex = re.compile(pattern)\n",
    "\n",
    "\n",
    "def tagged_user(string):\n",
    "    \"\"\"\n",
    "    replaces a user tagged with USER\n",
    "    \"\"\"\n",
    "    string = re.sub(tagged_user_regex, \" USER \", string)\n",
    "    return string\n",
    "\n",
    "\n",
    "function_list.append(tagged_user)\n",
    "\n",
    "pattern = r\"[^\\s]+\\.(com|org|net|gov|edu)[^\\s]*\"\n",
    "url_regex = re.compile(pattern)\n",
    "\n",
    "\n",
    "def urls(string):\n",
    "    \"\"\"\n",
    "    replaces URLs with URL\n",
    "    \"\"\"\n",
    "    string = re.sub(url_regex, \" URL \", string)\n",
    "    return string\n",
    "\n",
    "\n",
    "function_list.append(urls)\n",
    "\n",
    "pattern = r\"[\\r\\n]+\"\n",
    "enter_regex = re.compile(pattern, re.DOTALL)\n",
    "\n",
    "\n",
    "def enters(string):\n",
    "    \"\"\"\n",
    "    replaces \\r\\n with ENTER\n",
    "    \"\"\"\n",
    "    string = re.sub(enter_regex, \" \", string)\n",
    "    return string\n",
    "\n",
    "\n",
    "function_list.append(enters)\n",
    "\n",
    "pattern = r\"#####\"\n",
    "bold_regex = re.compile(pattern, re.DOTALL)\n",
    "\n",
    "\n",
    "def bold(string):\n",
    "    \"\"\"\n",
    "    replace bold characters with bold word\n",
    "    \"\"\"\n",
    "    string = re.sub(bold_regex, \" BOLD \", string)\n",
    "    return string\n",
    "\n",
    "\n",
    "function_list.append(bold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e16f68f8-cf89-4056-8dfb-27e40195c82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(string):\n",
    "    for func in function_list:\n",
    "        string = func(string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e05fdc6a-8210-4de0-ae86-35c5680b1897",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = set(punctuation)\n",
    "\n",
    "\n",
    "def all_punc(word):\n",
    "    for ch in word:\n",
    "        if ch not in punct:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bf6850",
   "metadata": {},
   "source": [
    "## Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24cccba",
   "metadata": {},
   "source": [
    "Here we tokenize each issue into words and stem each word and keep a count of how many mentions each word has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a56fe3f-bcda-4fc2-a9e1-0561a6071c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwds = set(stopwords.words(\"english\"))\n",
    "ps = PorterStemmer()\n",
    "counter = Counter()\n",
    "\n",
    "for i, row in issues_df.iterrows():\n",
    "    title = row.title\n",
    "    body = row.body\n",
    "    listed_words = nltk.word_tokenize(\n",
    "        preprocess(title + \" \" + body if type(body) == str else row.title).lower()\n",
    "    )\n",
    "    listed_words = [word for word in listed_words if not all_punc(word)]\n",
    "    stemmed = [ps.stem(word) for word in listed_words if word not in stopwds]\n",
    "    counter.update(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e24e40",
   "metadata": {},
   "source": [
    "We keep the most popular words that make up the top x% of the data, then save these words to ceph. This is for data reduction, since we are using SVMs with a small amount of data we want to keep the complexity of the input low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0535d281-7856-4b98-896a-dba69b6dccdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34235 unique words\n",
      "1077 unique words kept\n"
     ]
    }
   ],
   "source": [
    "# number of distinct words detected\n",
    "\n",
    "print(f\"{len(counter)} unique words\")\n",
    "\n",
    "# keep only the words that comprise 80% of the data (cut off tail)\n",
    "\n",
    "thresh = 0.8\n",
    "\n",
    "to_keep = dict()\n",
    "num_words = sum(counter.values())\n",
    "thresh = thresh * num_words\n",
    "curr = 0\n",
    "for word, val in counter.most_common():\n",
    "    if curr > thresh:\n",
    "        break\n",
    "    else:\n",
    "        to_keep[word] = val\n",
    "        curr += val\n",
    "\n",
    "print(f\"{len(to_keep)} unique words kept\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cef2103-07f4-48a4-ba80-deea77f6aba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('inlin', 10185),\n",
       " ('url', 9896),\n",
       " ('bold', 9422),\n",
       " ('code_block', 8048),\n",
       " ('use', 5476),\n",
       " ('openshift', 5446),\n",
       " ('user', 5199),\n",
       " ('creat', 4503),\n",
       " ('oc', 4497),\n",
       " ('error', 4479),\n",
       " ('version', 4399),\n",
       " ('result', 4396),\n",
       " ('info', 4348),\n",
       " ('build', 4327),\n",
       " ('pod', 4230),\n",
       " ('run', 4203),\n",
       " ('imag', 4034),\n",
       " ('permiss', 3815),\n",
       " ('fail', 3598),\n",
       " ('get', 3502),\n",
       " ('issu', 3398),\n",
       " ('deploy', 3283),\n",
       " (\"n't\", 3088),\n",
       " ('current', 2872),\n",
       " ('step', 2741),\n",
       " ('expect', 2580),\n",
       " ('extra', 2553),\n",
       " ('docker', 2490),\n",
       " ('test', 2411),\n",
       " ('log', 2400),\n",
       " ('command', 2384),\n",
       " ('cluster', 2366),\n",
       " ('contain', 2354),\n",
       " ('servic', 2296),\n",
       " ('kubernet', 2230),\n",
       " ('reproduc', 2217),\n",
       " ('work', 2189),\n",
       " ('node', 2162),\n",
       " ('start', 2071),\n",
       " ('tri', 1997),\n",
       " ('resourc', 1978),\n",
       " ('server', 1966),\n",
       " ('1', 1885),\n",
       " ('instal', 1866),\n",
       " ('name', 1815),\n",
       " ('1.', 1767),\n",
       " ('project', 1725),\n",
       " (\"'s\", 1661),\n",
       " ('registri', 1612),\n",
       " ('master', 1596)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at most popular stemmed words\n",
    "list(to_keep.items())[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecc9a044-11c9-4d2c-8e2a-53467a59926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../wordlist.txt\", \"w\") as f:\n",
    "    for word in list(to_keep):\n",
    "        f.write(word)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "948052d3-fc54-4678-8bbe-b516c648d32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_ceph:\n",
    "    s3.upload_file(\n",
    "        Bucket=s3_bucket, Key=\"github-labeler/wordlist.txt\", Filename=\"../wordlist.txt\"\n",
    "    )\n",
    "    os.remove(\"../wordlist.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
