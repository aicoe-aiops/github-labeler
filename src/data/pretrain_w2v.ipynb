{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33a41d22",
   "metadata": {},
   "source": [
    "In this notebook one can use all the issues made on github to train a Word2Vec model. This pre-trained W2V model can then be fed into a fastText model to classify github issues. This notebook includes gathering the data, processing the data inplace, and training the model with the loaded data. Since data can be streamed into the model one day at a time, no data has to be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49e910df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/atersaak/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "import urllib\n",
    "import zipfile\n",
    "import os\n",
    "import langid\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import unicodedata as ud\n",
    "from sklearn.decomposition import PCA\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d2b805",
   "metadata": {},
   "source": [
    "First a google cloud project has to be made in order to use BigQUery to access the [GHArchive](https://www.gharchive.org/). The service account credentials can be stored in the root folder and the project id should match the one below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7490ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save key .json file in the github labeler root\n",
    "# project id on bigquery account should match\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    '../../github-issue-data-extraction-key.json')\n",
    "\n",
    "project_id = 'github-issue-data-extraction'\n",
    "client = bigquery.Client(credentials= credentials, project=project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0851bc6f",
   "metadata": {},
   "source": [
    "We define some simple functions to get the data in the format we wish, removing the quotes around text and deleting issues made by bots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fd56814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple preprocessing functions\n",
    "\n",
    "def remove_quotes(string):\n",
    "    \"\"\"\n",
    "    Remove quotes from the string (everything extracted from json has quotes)\n",
    "    \"\"\"\n",
    "    if type(string) == str:\n",
    "        return string[1:-1]\n",
    "    else:\n",
    "        return string\n",
    "\n",
    "\n",
    "def is_bot(actor):\n",
    "    \"\"\"\n",
    "    Identify users clearly tagged as bots\n",
    "    \"\"\"\n",
    "    if type(actor) != str:\n",
    "        return True\n",
    "    if actor[-5:] == '[bot]':\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88b27b4",
   "metadata": {},
   "source": [
    "We make a function that can take in a given day and return a dataframe of the github issues made on that day, witht the light processing applied above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff8b66a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_day(day):\n",
    "    \"\"\"\n",
    "    Pass in a datetime object and a dataframe of all the issue data from that day will be returned\n",
    "    \"\"\"\n",
    "    date = day.strftime('%Y%m%d')\n",
    "    response = client.query(f\"\"\"SELECT JSON_EXTRACT(payload, '$.issue.title') as title,\n",
    "                                JSON_EXTRACT(payload, '$.issue.body') as body,\n",
    "                                JSON_EXTRACT(payload, '$.issue.html_url') as url,\n",
    "                                JSON_EXTRACT(payload, '$.issue.user.login') as actor\n",
    "                                FROM githubarchive.day.{date}\n",
    "                                WHERE type = 'IssuesEvent' AND JSON_EXTRACT(payload, '$.action') = '\"opened\"'\n",
    "                                \"\"\")\n",
    "    df = response.to_dataframe()\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_df(df):\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].apply(remove_quotes)\n",
    "        df = df[~df[col].apply(is_bot)]\n",
    "    df = df[~df[col].apply(is_bot)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dc987c",
   "metadata": {},
   "source": [
    "Now we define a slew of preprocessing functions that simplify the text data and make it easier for the Word2Vec models to understand them. We also use langid to check if the language of the issue is in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9d99972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english(text):\n",
    "    \"\"\"\n",
    "    Determine if a language is English\n",
    "    \"\"\"\n",
    "    return langid.classify(text)[0] == 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2181641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### preprocess functions defined below\n",
    "\n",
    "function_list = []\n",
    "\n",
    "pattern = r\"```.+?```\"\n",
    "code_block_regex = re.compile(pattern, re.DOTALL)\n",
    "\n",
    "\n",
    "def code_block(string):\n",
    "    \"\"\"Replace code blocks with a CODE_BLOCK.\"\"\"\n",
    "    string = re.sub(code_block_regex, \"CODE_BLOCK\", string)\n",
    "    return string\n",
    "\n",
    "\n",
    "function_list.append(code_block)\n",
    "\n",
    "pattern = r\"`{1,2}.+?`{1,2}\"\n",
    "inline_code_regex = re.compile(pattern, re.DOTALL)\n",
    "\n",
    "\n",
    "def code_variable(string):\n",
    "    \"\"\"Replace inline code with INLINE.\"\"\"\n",
    "    string = re.sub(inline_code_regex, \" INLINE \", string)\n",
    "    return string\n",
    "\n",
    "\n",
    "function_list.append(code_variable)\n",
    "\n",
    "pattern = r\"\\s@[^\\s]+\"\n",
    "tagged_user_regex = re.compile(pattern)\n",
    "\n",
    "\n",
    "def tagged_user(string):\n",
    "    \"\"\"Replace a user tagged with USER.\"\"\"\n",
    "    string = re.sub(tagged_user_regex, \" USER \", string)\n",
    "    return string\n",
    "\n",
    "\n",
    "function_list.append(tagged_user)\n",
    "\n",
    "pattern = r\"[^\\s]+\\.(com|org|net|gov|edu|io|ai)[^\\s]*\"\n",
    "url_regex = re.compile(pattern)\n",
    "\n",
    "\n",
    "def urls(string):\n",
    "    \"\"\"Replace URLs with URL.\"\"\"\n",
    "    string = re.sub(url_regex, \" URL \", string)\n",
    "    return string\n",
    "\n",
    "\n",
    "function_list.append(urls)\n",
    "\n",
    "pattern = r\"((\\\\r)*\\\\n)+\"\n",
    "enter_regex = re.compile(pattern, re.DOTALL)\n",
    "\n",
    "\n",
    "def enters(string):\n",
    "    \"\"\"Replace newline characters with a space.\"\"\"\n",
    "    string = re.sub(enter_regex, \" \", string)\n",
    "    return string\n",
    "\n",
    "\n",
    "function_list.append(enters)\n",
    "\n",
    "pattern = r\"#{3,}\"\n",
    "bold_regex = re.compile(pattern, re.DOTALL)\n",
    "\n",
    "\n",
    "def bold(string):\n",
    "    \"\"\"Replace bold characters with a space.\"\"\"\n",
    "    string = re.sub(bold_regex, \" \", string)\n",
    "    return string\n",
    "\n",
    "\n",
    "function_list.append(bold)\n",
    "\n",
    "\n",
    "def remove_slashes(string):\n",
    "    return string.replace('\\\\', '')\n",
    "\n",
    "\n",
    "function_list.append(remove_slashes)\n",
    "\n",
    "\n",
    "def preprocess(string):\n",
    "    \"\"\"Put all preprocessing functions together.\"\"\"\n",
    "    for func in function_list:\n",
    "        string = func(string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9da14939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that will remove all punctuation that is not ending a sentence or a comma\n",
    "\n",
    "punc = set(punctuation)\n",
    "\n",
    "\n",
    "def is_punc(string):\n",
    "    if string in ['.', '?', '!', ',']:\n",
    "        return False\n",
    "    for ch in string:\n",
    "        if ch not in punc:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787c8e66",
   "metadata": {},
   "source": [
    "Now we will extract the github data to create a vocabulary set. A certain number of days can be specified here, and the data will begin from issues a week ago and continue extracting one day from every two weeks. The data is not saved, but the word counts are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8b4c68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# here we download some data spaced out over about two years to build vocabulary\n",
    "\n",
    "\n",
    "total_data = 0\n",
    "\n",
    "curr_day = datetime.datetime.today().date() - datetime.timedelta(days = 7)\n",
    "\n",
    "num_days = 1\n",
    "\n",
    "cnt = Counter()\n",
    "\n",
    "while num_days < 50:\n",
    "    df = get_data_for_day(curr_day)\n",
    "    df = process_df(df)\n",
    "    inp = df['title'].fillna(' ') + ' SEP ' + df['body'].fillna(' ')\n",
    "    inp = inp.apply(preprocess)\n",
    "    inp = inp[inp.apply(is_english)]\n",
    "    inp = inp.apply(lambda x: x.lower())\n",
    "    inp = inp.apply(word_tokenize).values\n",
    "    inp = [[word for word in issue if not is_punc(word)] for issue in inp]\n",
    "    inp = [set(words) for words in inp]\n",
    "    total_data += sum(df.memory_usage(deep = True))/1000000000\n",
    "    if (num_days + 1) % 3 == 0:\n",
    "        print(f'{num_days} days and {round(total_data, 2)} GB looked at')\n",
    "    curr_day -= datetime.timedelta(days = 14)\n",
    "    num_days += 1\n",
    "    for d in inp:\n",
    "        cnt.update(d)\n",
    "\n",
    "print(f'{len(cnt)} total words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1532888b",
   "metadata": {},
   "source": [
    "Now we download the pretrained model that was trained on wikipedia and the news. We delete noisy words that are unlikely to come up to reduce the size of the model using some criteria. We then add in the words extracted from issues that comprise 95% of all words that don't already exist in our dataset. We use PCA to reduce the vector size of the words from the pretrained model. We do this by looking at the sum of the singular values and making sure 70% of the sum of the SV's are covered by the reduced data's SV's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e042e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pretrained english model\n",
    "\n",
    "if not os.path.isfile('../models/wiki-news-300d-1M.vec'):\n",
    "    urllib.urlretrieve(\"https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\",\n",
    "                       \"../models/wiki-news-300d-1M.vec.zip\")\n",
    "    with zipfile.ZipFile('../models/wiki-news-300d-1M.vec.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('../models')\n",
    "    os.remove('../models/wiki-news-300d-1M.vec.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18d8f660",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('../models/wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fec0105",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999994\n",
      "392610\n",
      "296866\n",
      "284422\n",
      "244220\n"
     ]
    }
   ],
   "source": [
    "print(len(model.vocab))\n",
    "# remove capital letters\n",
    "pretrained_vocab = [v for v in model.vocab.keys() if v.lower() == v]\n",
    "print(len(pretrained_vocab))\n",
    "# remove bigrams\n",
    "pretrained_vocab = [v for v in pretrained_vocab if len(v.split('-')) < 2]\n",
    "print(len(pretrained_vocab))\n",
    "# remove words with nonlatin characters\n",
    "\n",
    "# from stackexchange\n",
    "\n",
    "latin_letters= {}\n",
    "\n",
    "\n",
    "def is_latin(uchr):\n",
    "    try:\n",
    "        return latin_letters[uchr]\n",
    "    except KeyError:\n",
    "        return latin_letters.setdefault(uchr, 'LATIN' in ud.name(uchr))\n",
    "\n",
    "\n",
    "def only_roman_chars(unistr):\n",
    "    return all(is_latin(uchr) for uchr in unistr if uchr.isalpha())\n",
    "\n",
    "\n",
    "pretrained_vocab = [v for v in pretrained_vocab if only_roman_chars(v)]\n",
    "\n",
    "print(len(pretrained_vocab))\n",
    "\n",
    "digits = set('0123456789')\n",
    "\n",
    "\n",
    "def is_mostly_numeric(string):\n",
    "    cnt = 0\n",
    "    for s in string:\n",
    "        if s in digits:\n",
    "            cnt += 1\n",
    "    if cnt/len(string) > .5:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "pretrained_vocab = [v for v in pretrained_vocab if not is_mostly_numeric(v)]\n",
    "\n",
    "print(len(pretrained_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51d941c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142484 words added from random github issues\n"
     ]
    }
   ],
   "source": [
    "vocab_set =set(pretrained_vocab)\n",
    "total_num = sum([b for a, b in cnt.most_common()])\n",
    "top_words = []\n",
    "cutoff = 0.95*total_num\n",
    "running = 0\n",
    "for word, num in cnt.most_common():\n",
    "    if running < cutoff:\n",
    "        if word not in vocab_set:\n",
    "            top_words.append(word)\n",
    "        running += num\n",
    "    else:\n",
    "        break\n",
    "del cnt\n",
    "print(f'{len(top_words)} words added from random github issues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63d5a17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175 components explain 0.7 of the variation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PCA(n_components=175)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce size\n",
    "\n",
    "data = np.stack([model[v] for v in pretrained_vocab])\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "pca.fit(data)\n",
    "\n",
    "total_singular_values = sum(pca.singular_values_)\n",
    "\n",
    "thresh = 0.7\n",
    "running = 0\n",
    "n_comps = 0\n",
    "while running < thresh*total_singular_values:\n",
    "    running += pca.singular_values_[n_comps]\n",
    "    n_comps += 1\n",
    "\n",
    "print(f'{n_comps} components explain {thresh} of the variation')\n",
    "\n",
    "final_pca = PCA(n_components = n_comps)\n",
    "final_pca.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e02a58",
   "metadata": {},
   "source": [
    "Now we initialize our Word2Vec model and build the vocabulary. We join the resulting words from the pretrained model, add in the new words discovered from the issue data, as well as an \"unknown\" character. We load in the pretrained vocabulary into the Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b175a6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atersaak/anaconda3/lib/python3.8/site-packages/gensim/models/base_any2vec.py:742: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "w = Word2Vec(size=n_comps, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a4b82aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w.build_vocab(sentences = [pretrained_vocab + top_words + ['_unknown_']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17a8c652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a07ffd3d924461a148bab7023ca5cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/244220 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for v in tqdm(pretrained_vocab):\n",
    "    w.wv[v] = final_pca.transform([model[v]])[0]\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46962a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_set(word):\n",
    "    if word in w.wv:\n",
    "        return word\n",
    "    else:\n",
    "        return '_unknown_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99eb307",
   "metadata": {},
   "source": [
    "Finally, we train Word2Vec on 50 days of data, reading in one day at a time. We start 10 days ago and shift back 2 weeks with each iteration. The model saves after each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b67e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 days completed\n",
      "2 days completed\n",
      "3 days completed\n",
      "4 days completed\n",
      "5 days completed\n",
      "6 days completed\n"
     ]
    }
   ],
   "source": [
    "curr_day = datetime.datetime.today().date() - datetime.timedelta(days = 10)\n",
    "\n",
    "num_days = 1\n",
    "\n",
    "while num_days < 50:\n",
    "    df = get_data_for_day(curr_day)\n",
    "    df = process_df(df)\n",
    "    df['proc'] = df['title'].fillna(' ') + ' SEP ' + df['body'].fillna(' ')\n",
    "    df['proc'] = df['proc'].apply(preprocess)\n",
    "    df = df[df['proc'].apply(is_english)]\n",
    "    df['proc'] = df['proc'].apply(lambda x: x.lower())\n",
    "    inp = df['proc'].apply(word_tokenize).values\n",
    "    inp = [[in_set(word) for word in issue if not is_punc(word)] for issue in inp]\n",
    "    curr_day -= datetime.timedelta(days = 14)\n",
    "    w.save('w2v.model')\n",
    "    print(f'{num_days} days completed')\n",
    "    num_days += 1\n",
    "    w.train(inp, total_examples = len(inp), epochs = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
