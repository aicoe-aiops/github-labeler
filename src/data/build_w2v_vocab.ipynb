{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33a41d22",
   "metadata": {},
   "source": [
    "In this notebook one can use all the issues made on github to train a Word2Vec model. This pre-trained W2V model can then be fed into a fastText model to classify github issues. This notebook includes gathering the data, processing the data inplace, and training the model with the loaded data. Since data can be streamed into the model one day at a time, no data has to be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49e910df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/atersaak/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import datetime\n",
    "import numpy as np\n",
    "import urllib\n",
    "import zipfile\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import unicodedata as ud\n",
    "from sklearn.decomposition import PCA\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import boto3\n",
    "from w2v_preprocess import remove_quotes, is_bot, is_english, preprocess, is_punc\n",
    "\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a5e3eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a64d120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whether to use ceph or store locally\n",
    "\n",
    "use_ceph = True\n",
    "\n",
    "if use_ceph:\n",
    "    s3_endpoint_url = os.environ[\"OBJECT_STORAGE_ENDPOINT_URL\"]\n",
    "    s3_access_key = os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
    "    s3_secret_key = os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "    s3_bucket = os.environ[\"OBJECT_STORAGE_BUCKET_NAME\"]\n",
    "\n",
    "    s3 = boto3.client(\n",
    "        service_name=\"s3\",\n",
    "        aws_access_key_id=s3_access_key,\n",
    "        aws_secret_access_key=s3_secret_key,\n",
    "        endpoint_url=s3_endpoint_url,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d2b805",
   "metadata": {},
   "source": [
    "First a google cloud project has to be made in order to use BigQuery to access the [GHArchive](https://www.gharchive.org/). The service account credentials can be stored in the root folder and the project id should match the one below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7490ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save key .json file in the github labeler root\n",
    "# project id on bigquery account should match\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    '../../github-issue-data-extraction-key.json')\n",
    "\n",
    "project_id = 'github-issue-data-extraction'\n",
    "client = bigquery.Client(credentials= credentials, project=project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88b27b4",
   "metadata": {},
   "source": [
    "We make a function that can take in a given day and return a dataframe of the github issues made on that day, witht the light processing applied above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff8b66a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_day(day):\n",
    "    \"\"\"\n",
    "    Pass in a datetime object and a dataframe of all the issue data from that day will be returned\n",
    "    \"\"\"\n",
    "    date = day.strftime('%Y%m%d')\n",
    "    response = client.query(f\"\"\"SELECT JSON_EXTRACT(payload, '$.issue.title') as title,\n",
    "                                JSON_EXTRACT(payload, '$.issue.body') as body,\n",
    "                                JSON_EXTRACT(payload, '$.issue.html_url') as url,\n",
    "                                JSON_EXTRACT(payload, '$.issue.user.login') as actor\n",
    "                                FROM githubarchive.day.{date}\n",
    "                                WHERE type = 'IssuesEvent' AND JSON_EXTRACT(payload, '$.action') = '\"opened\"'\n",
    "                                \"\"\")\n",
    "    df = response.to_dataframe()\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_df(df):\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].apply(remove_quotes)\n",
    "        df = df[~df[col].apply(is_bot)]\n",
    "    df = df[~df[col].apply(is_bot)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787c8e66",
   "metadata": {},
   "source": [
    "Now we will extract the github data to create a vocabulary set. A certain number of days can be specified here, and the data will begin from issues a week ago and continue extracting one day from every two weeks. The data is not saved, but the word counts are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8b4c68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# here we download some data spaced out over about two years to build vocabulary\n",
    "\n",
    "\n",
    "total_data = 0\n",
    "\n",
    "curr_day = datetime.datetime.today().date() - datetime.timedelta(days = 7)\n",
    "\n",
    "num_days = 1\n",
    "\n",
    "cnt = Counter()\n",
    "\n",
    "while num_days < 50:\n",
    "    df = get_data_for_day(curr_day)\n",
    "    df = process_df(df)\n",
    "    inp = df['title'].fillna(' ') + ' SEP ' + df['body'].fillna(' ')\n",
    "    inp = inp.apply(preprocess)\n",
    "    inp = inp[inp.apply(is_english)]\n",
    "    inp = inp.apply(lambda x: x.lower())\n",
    "    inp = inp.apply(word_tokenize).values\n",
    "    inp = [[word for word in issue if not is_punc(word)] for issue in inp]\n",
    "    inp = [set(words) for words in inp]\n",
    "    total_data += sum(df.memory_usage(deep = True))/1000000000\n",
    "    if (num_days + 1) % 3 == 0:\n",
    "        print(f'{num_days} days and {round(total_data, 2)} GB looked at')\n",
    "    curr_day -= datetime.timedelta(days = 14)\n",
    "    num_days += 1\n",
    "    for d in inp:\n",
    "        cnt.update(d)\n",
    "\n",
    "print(f'{len(cnt)} total words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1532888b",
   "metadata": {},
   "source": [
    "Now we download the pretrained model that was trained on wikipedia and the news. We delete noisy words that are unlikely to come up to reduce the size of the model using some criteria. We then add in the words extracted from issues that comprise 95% of all words that don't already exist in our dataset. We use PCA to reduce the vector size of the words from the pretrained model. We do this by looking at the sum of the singular values and making sure 70% of the sum of the SV's are covered by the reduced data's SV's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e042e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pretrained english model\n",
    "\n",
    "if not os.path.isfile('../models/wiki-news-300d-1M.vec'):\n",
    "    urllib.urlretrieve(\"https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\",\n",
    "                       \"../models/wiki-news-300d-1M.vec.zip\")\n",
    "    with zipfile.ZipFile('../models/wiki-news-300d-1M.vec.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('../models')\n",
    "    os.remove('../models/wiki-news-300d-1M.vec.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18d8f660",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('../models/wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fec0105",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999994\n",
      "392610\n",
      "296866\n",
      "284422\n",
      "244220\n"
     ]
    }
   ],
   "source": [
    "print(len(model.vocab))\n",
    "# remove capital letters\n",
    "pretrained_vocab = [v for v in model.vocab.keys() if v.lower() == v]\n",
    "print(len(pretrained_vocab))\n",
    "# remove bigrams\n",
    "pretrained_vocab = [v for v in pretrained_vocab if len(v.split('-')) < 2]\n",
    "print(len(pretrained_vocab))\n",
    "# remove words with nonlatin characters\n",
    "\n",
    "# from stackexchange\n",
    "\n",
    "latin_letters= {}\n",
    "\n",
    "\n",
    "def is_latin(uchr):\n",
    "    try:\n",
    "        return latin_letters[uchr]\n",
    "    except KeyError:\n",
    "        return latin_letters.setdefault(uchr, 'LATIN' in ud.name(uchr))\n",
    "\n",
    "\n",
    "def only_roman_chars(unistr):\n",
    "    return all(is_latin(uchr) for uchr in unistr if uchr.isalpha())\n",
    "\n",
    "\n",
    "pretrained_vocab = [v for v in pretrained_vocab if only_roman_chars(v)]\n",
    "\n",
    "print(len(pretrained_vocab))\n",
    "\n",
    "digits = set('0123456789')\n",
    "\n",
    "\n",
    "def is_mostly_numeric(string):\n",
    "    cnt = 0\n",
    "    for s in string:\n",
    "        if s in digits:\n",
    "            cnt += 1\n",
    "    if cnt/len(string) > .5:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "pretrained_vocab = [v for v in pretrained_vocab if not is_mostly_numeric(v)]\n",
    "\n",
    "print(len(pretrained_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51d941c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142484 words added from random github issues\n"
     ]
    }
   ],
   "source": [
    "vocab_set =set(pretrained_vocab)\n",
    "total_num = sum([b for a, b in cnt.most_common()])\n",
    "top_words = []\n",
    "cutoff = 0.95*total_num\n",
    "running = 0\n",
    "for word, num in cnt.most_common():\n",
    "    if running < cutoff:\n",
    "        if word not in vocab_set:\n",
    "            top_words.append(word)\n",
    "        running += num\n",
    "    else:\n",
    "        break\n",
    "del cnt\n",
    "print(f'{len(top_words)} words added from random github issues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63d5a17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175 components explain 0.7 of the variation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PCA(n_components=175)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce size\n",
    "\n",
    "data = np.stack([model[v] for v in pretrained_vocab])\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "pca.fit(data)\n",
    "\n",
    "total_singular_values = sum(pca.singular_values_)\n",
    "\n",
    "thresh = 0.7\n",
    "running = 0\n",
    "n_comps = 0\n",
    "while running < thresh*total_singular_values:\n",
    "    running += pca.singular_values_[n_comps]\n",
    "    n_comps += 1\n",
    "\n",
    "print(f'{n_comps} components explain {thresh} of the variation')\n",
    "\n",
    "final_pca = PCA(n_components = n_comps)\n",
    "final_pca.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e02a58",
   "metadata": {},
   "source": [
    "Now we initialize our Word2Vec model and build the vocabulary. We join the resulting words from the pretrained model, add in the new words discovered from the issue data, as well as an \"unknown\" character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b175a6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atersaak/anaconda3/lib/python3.8/site-packages/gensim/models/base_any2vec.py:742: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "w = Word2Vec(size=n_comps, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a4b82aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w.build_vocab(sentences = [pretrained_vocab + top_words + ['_unknown_']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17a8c652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a07ffd3d924461a148bab7023ca5cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/244220 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for v in tqdm(pretrained_vocab):\n",
    "    w.wv[v] = final_pca.transform([model[v]])[0]\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfc5171",
   "metadata": {},
   "source": [
    "We save the model in Ceph. Since it stores multiple files, we must account for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e9fe60d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.save('../../models/w2v.model')\n",
    "\n",
    "if use_ceph:\n",
    "    for file in os.listdir('../../models/'):\n",
    "        if 'w2v.model' in file:\n",
    "            response = s3.upload_file(\n",
    "                Bucket=s3_bucket,\n",
    "                Key=f\"github-labeler/w2v/{file}\",\n",
    "                Filename=f'../../models/{file}',\n",
    "            )\n",
    "            os.remove(f'../../models/{file}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
